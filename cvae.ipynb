{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import make_moons\n",
    "from tqdm import tqdm, trange\n",
    "import torch.distributions as dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(nn.Module):\n",
    "    \"\"\"Conditional Variational Autoencoder (CVAE) class.\n",
    "\n",
    "    Args:\n",
    "        x_dim (int): Dimensionality of the condition x.\n",
    "        y_dim (int): Dimensionality of the input/output data y.\n",
    "        hidden_dim (int): Dimensionality of the hidden layers.\n",
    "        latent_dim (int): Dimensionality of the latent space.\n",
    "    \"\"\"\n",
    "    def __init__(self, x_dim, y_dim, hidden_dim, latent_dim):\n",
    "        super(CVAE, self).__init__()\n",
    "\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # 1.1 Modelo de Inferência (Encoder Variacional) q(z | x, y)\n",
    "        self.inference_network = nn.Sequential(\n",
    "            nn.Linear(x_dim + y_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 2 * latent_dim)  # mu_q e log_sigma_q^2\n",
    "        )\n",
    "\n",
    "        # 1.2 Modelo da Priori Condicionada p(z | x)\n",
    "        self.prior_network = nn.Sequential(\n",
    "            nn.Linear(x_dim, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 2 * latent_dim)  # mu_p e log_sigma_p^2\n",
    "        )\n",
    "\n",
    "        # 1.3 Modelo Gerador (Decoder) p(y | x, z)\n",
    "        self.generator_network = nn.Sequential(\n",
    "            nn.Linear(x_dim + latent_dim, hidden_dim // 2),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(hidden_dim, y_dim)  # mean de y\n",
    "        )\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Encoder q(z|x,y)\n",
    "        xy = torch.cat([x, y], dim=1)\n",
    "        params_q = self.inference_network(xy)\n",
    "        mu_q = params_q[:, :self.latent_dim]\n",
    "        log_sigma_sq_q = params_q[:, self.latent_dim:]\n",
    "        q_dist = dist.Normal(mu_q, torch.exp(0.5 * log_sigma_sq_q))\n",
    "\n",
    "        # Reparameterization\n",
    "        z = q_dist.rsample()\n",
    "\n",
    "        # Decoder p(y|x,z)\n",
    "        xz = torch.cat([x, z], dim=1)\n",
    "        y_recon_mean = self.generator_network(xz)\n",
    "\n",
    "        return y_recon_mean, q_dist, z\n",
    "\n",
    "    def compute_loss(self, x, y, kl_weight=1.0):\n",
    "        y_recon_mean, q_dist, z = self.forward(x, y)\n",
    "\n",
    "        # Reconstruction Loss (MSE, equivalente a Gaussian NLL com variância fixa)\n",
    "        reconstruction_loss = nn.functional.mse_loss(y_recon_mean, y, reduction='sum')\n",
    "\n",
    "        # KL Divergence\n",
    "        # Prior p(z|x)\n",
    "        params_p = self.prior_network(x)\n",
    "        mu_p = params_p[:, :self.latent_dim]\n",
    "        log_sigma_sq_p = params_p[:, self.latent_dim:]\n",
    "        p_dist = dist.Normal(mu_p, torch.exp(0.5 * log_sigma_sq_p))\n",
    "\n",
    "        kl_loss = dist.kl_divergence(q_dist, p_dist).sum()\n",
    "\n",
    "        # Loss final\n",
    "        loss = reconstruction_loss + kl_weight * kl_loss\n",
    "\n",
    "        return loss, reconstruction_loss, kl_loss\n",
    "\n",
    "    def generate(self, x):\n",
    "        # Amostra z da priori p(z|x) e decodifica para gerar y\n",
    "        params_p = self.prior_network(x)\n",
    "        mu_p = params_p[:, :self.latent_dim]\n",
    "        log_sigma_sq_p = params_p[:, self.latent_dim:]\n",
    "        p_dist = dist.Normal(mu_p, torch.exp(0.5 * log_sigma_sq_p))\n",
    "        z = p_dist.sample()\n",
    "        xz = torch.cat([x, z], dim=1)\n",
    "        y_gen = self.generator_network(xz)\n",
    "        return y_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Moons(Dataset):\n",
    "    def __init__(self, num_samples: int, noise: float):\n",
    "        super().__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self._X, self._y = make_moons(n_samples=num_samples, noise=noise, random_state=123)\n",
    "        self.X = torch.from_numpy(self._X).float()\n",
    "        self.y_class = torch.from_numpy(self._y.reshape(-1, 1)).float()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # x é a primeira coordenada, y é a segunda\n",
    "        return self.X[index, 0].unsqueeze(0), self.X[index, 1].unsqueeze(0), self.y_class[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size = 150\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 300\n",
    "x_dim = 1\n",
    "y_dim = 1\n",
    "latent_dim = 2\n",
    "hidden_dim = 256\n",
    "kl_weight = 0.1 # Beta para o beta-VAE\n",
    "\n",
    "train_data = Moons(3000, noise=0.15)\n",
    "test_data = Moons(600, noise=0.15)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, kl_weight):\n",
    "    model.train()\n",
    "    total_loss, total_recon_loss, total_kl_loss = 0.0, 0.0, 0.0\n",
    "    for x, y, _ in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss, recon_loss, kl_loss = model.compute_loss(x, y, kl_weight)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        total_recon_loss += recon_loss.item()\n",
    "        total_kl_loss += kl_loss.item()\n",
    "    return total_loss / len(dataloader.dataset), total_recon_loss / len(dataloader.dataset), total_kl_loss / len(dataloader.dataset)\n",
    "\n",
    "def test(model, dataloader, kl_weight):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y, _ in dataloader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            loss, _, _ = model.compute_loss(x, y, kl_weight)\n",
    "            test_loss += loss.item()\n",
    "    return test_loss / len(dataloader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Conditional Variational Autoencoder...\")\n",
    "model_CVAE = CVAE(x_dim=x_dim, y_dim=y_dim, hidden_dim=hidden_dim, latent_dim=latent_dim).to(device)\n",
    "print(model_CVAE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model_CVAE.parameters(), lr=learning_rate)\n",
    "\n",
    "train_loss_history_CVAE = []\n",
    "test_loss_history_CVAE = []\n",
    "for epoch in trange(1, num_epochs + 1, desc='Training', unit='epoch'):\n",
    "    train_loss, recon_loss, kl_loss = train(model_CVAE, train_loader, optimizer, kl_weight)\n",
    "    test_loss = test(model_CVAE, test_loader, kl_weight)\n",
    "    train_loss_history_CVAE.append(train_loss)\n",
    "    test_loss_history_CVAE.append(test_loss)\n",
    "    if epoch % 20 == 0:\n",
    "        print(f'Epoch {epoch}, Train Loss: {train_loss:.4f}, Recon Loss: {recon_loss:.4f}, KL Loss: {kl_loss:.4f}, Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(train_loss_history_CVAE)+1), train_loss_history_CVAE, label=\"Train loss (epoch average)\")\n",
    "plt.plot(range(1, len(train_loss_history_CVAE)+1), test_loss_history_CVAE, label=\"Test loss\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Conditional Variational Autoencoder\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_space_cvae(model, dataloader):\n",
    "    model.eval()\n",
    "    z_all = []\n",
    "    y_class_all = []\n",
    "    with torch.no_grad():\n",
    "        for x, y, y_class in tqdm(dataloader, desc='Encoding', unit='batch'):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            xy = torch.cat([x, y], dim=1)\n",
    "            params_q = model.inference_network(xy)\n",
    "            mu_q = params_q[:, :model.latent_dim]\n",
    "            z_all.append(mu_q.cpu().numpy())\n",
    "            y_class_all.append(y_class.numpy())\n",
    "    z_all = np.concatenate(z_all, axis=0)\n",
    "    y_class_all = np.concatenate(y_class_all, axis=0)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(z_all[:, 0], z_all[:, 1], c=y_class_all, cmap='tab10', alpha=0.5)\n",
    "    plt.title('Latent Space Projection (Mean)')\n",
    "    plt.xlabel('z1')\n",
    "    plt.ylabel('z2')\n",
    "    plt.show()\n",
    "\n",
    "plot_latent_space_cvae(model_CVAE, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_generated_samples_cvae(model, dataloader, num_samples=1000):\n",
    "    model.eval()\n",
    "    \n",
    "    # Pega uma amostra de x do dataloader para usar como condição\n",
    "    x_cond, _, _ = next(iter(dataloader))\n",
    "    x_cond = x_cond.to(device)\n",
    "    # Repete as condições para gerar várias amostras para cada x\n",
    "    x_cond = x_cond.repeat(num_samples // x_cond.size(0) + 1, 1)[:num_samples]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_gen = model.generate(x_cond)\n",
    "    \n",
    "    x_cond_np = x_cond.cpu().numpy()\n",
    "    y_gen_np = y_gen.cpu().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(x_cond_np, y_gen_np, alpha=0.5, label='Generated')\n",
    "    \n",
    "    # Plota os dados originais para comparação\n",
    "    original_data = dataloader.dataset.X.numpy()\n",
    "    plt.scatter(original_data[:, 0], original_data[:, 1], alpha=0.1, label='Original')\n",
    "    \n",
    "    plt.title('Generated Samples from CVAE')\n",
    "    plt.xlabel('x (condition)')\n",
    "    plt.ylabel('y (generated)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plot_generated_samples_cvae(model_CVAE, test_loader, num_samples=2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
